{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97eb723c",
   "metadata": {},
   "source": [
    "### Look Ahead CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f723b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tracemalloc\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c185a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Tree node for lookahead CART.\"\"\"\n",
    "    def __init__(self, depth, data_idx, y, parent=None, condition=None):\n",
    "        self.depth = depth\n",
    "        self.data_idx = data_idx\n",
    "        self.y = y\n",
    "        self.parent = parent\n",
    "        self.condition = condition  # (feature, threshold, is_left)\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.pred = self.y[data_idx].mean() if len(data_idx) > 0 else 0.0\n",
    "        self.mse = np.var(self.y[data_idx]) if len(data_idx) > 0 else 0.0\n",
    "\n",
    "class LookaheadCART:\n",
    "    \"\"\"CART with bounded lookahead search.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=3, min_leaf_samples=10, lookahead_depth=2, n_candidate_splits=10):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_samples = min_leaf_samples\n",
    "        self.lookahead_depth = lookahead_depth\n",
    "        self.n_candidate_splits = n_candidate_splits\n",
    "        self.evaluations = 0\n",
    "        self.pruned_branches = 0\n",
    "        self.fit_time = 0\n",
    "        self.peak_memory = 0\n",
    "        \n",
    "    def get_candidate_splits(self, X, feature):\n",
    "        \"\"\"Get candidate split points using quantiles.\"\"\"\n",
    "        vals = X[feature].values\n",
    "        if len(np.unique(vals)) <= self.n_candidate_splits:\n",
    "            return np.unique(vals)[:-1]  # All unique values except max\n",
    "        \n",
    "        quantiles = np.linspace(0.1, 0.9, self.n_candidate_splits)\n",
    "        candidates = np.quantile(vals, quantiles)\n",
    "        return np.unique(candidates)\n",
    "    \n",
    "    def calculate_mse_after_split(self, y, left_idx, right_idx):\n",
    "        \"\"\"Calculate weighted MSE after a split.\"\"\"\n",
    "        if len(left_idx) == 0 or len(right_idx) == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        left_pred = y[left_idx].mean()\n",
    "        right_pred = y[right_idx].mean()\n",
    "        \n",
    "        left_mse = np.mean((y[left_idx] - left_pred) ** 2)\n",
    "        right_mse = np.mean((y[right_idx] - right_pred) ** 2)\n",
    "        \n",
    "        total_samples = len(left_idx) + len(right_idx)\n",
    "        weighted_mse = (len(left_idx) * left_mse + len(right_idx) * right_mse) / total_samples\n",
    "        \n",
    "        return weighted_mse\n",
    "    \n",
    "    def find_best_split_greedy(self, X, y, data_idx):\n",
    "        \"\"\"Standard greedy split finding.\"\"\"\n",
    "        best_mse = float('inf')\n",
    "        best_split = None\n",
    "        \n",
    "        current_mse = np.var(y[data_idx])\n",
    "        \n",
    "        for feature in X.columns:\n",
    "            candidates = self.get_candidate_splits(X.iloc[data_idx], feature)\n",
    "            \n",
    "            for threshold in candidates:\n",
    "                mask = X.iloc[data_idx][feature] <= threshold\n",
    "                left_idx = data_idx[mask]\n",
    "                right_idx = data_idx[~mask]\n",
    "                \n",
    "                if len(left_idx) < self.min_leaf_samples or len(right_idx) < self.min_leaf_samples:\n",
    "                    continue\n",
    "                \n",
    "                mse = self.calculate_mse_after_split(y, left_idx, right_idx)\n",
    "                self.evaluations += 1\n",
    "                \n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_split = (feature, threshold, left_idx, right_idx)\n",
    "        \n",
    "        return best_split, best_mse\n",
    "    \n",
    "    def calculate_optimistic_bound(self, y, data_idx, remaining_depth):\n",
    "        \"\"\"Calculate optimistic bound on MSE reduction with perfect splits.\"\"\"\n",
    "        if remaining_depth <= 0 or len(data_idx) < 2 * self.min_leaf_samples:\n",
    "            return np.var(y[data_idx])\n",
    "        \n",
    "        current_var = np.var(y[data_idx])\n",
    "        # Optimistic assumption: each split level can reduce variance by 50%\n",
    "        # (obviously impossible in practice, but gives us a bound)\n",
    "        optimistic_var = current_var * (0.5 ** remaining_depth)\n",
    "        return max(0.0, optimistic_var)\n",
    "    \n",
    "    def find_best_split_lookahead(self, X, y, data_idx, depth, alpha, beta, tree_depth):\n",
    "        \"\"\"Find best split with lookahead and alpha-beta pruning.\"\"\"\n",
    "        if (depth <= 0 or \n",
    "            len(data_idx) < 2 * self.min_leaf_samples or \n",
    "            tree_depth >= self.max_depth):\n",
    "            return None, np.var(y[data_idx])\n",
    "        \n",
    "        best_split = None\n",
    "        best_score = float('inf')\n",
    "        \n",
    "        for feature in X.columns:\n",
    "            candidates = self.get_candidate_splits(X.iloc[data_idx], feature)\n",
    "            \n",
    "            for threshold in candidates:\n",
    "                mask = X.iloc[data_idx][feature] <= threshold\n",
    "                left_idx = data_idx[mask]\n",
    "                right_idx = data_idx[~mask]\n",
    "                \n",
    "                if len(left_idx) < self.min_leaf_samples or len(right_idx) < self.min_leaf_samples:\n",
    "                    continue\n",
    "                \n",
    "                # Immediate MSE\n",
    "                immediate_mse = self.calculate_mse_after_split(y, left_idx, right_idx)\n",
    "                self.evaluations += 1\n",
    "                \n",
    "                # Calculate total samples (needed for both lookahead and pruning)\n",
    "                total_samples = len(left_idx) + len(right_idx)\n",
    "                \n",
    "                if depth > 1:\n",
    "                    # Lookahead: evaluate future potential\n",
    "                    _, left_future = self.find_best_split_lookahead(\n",
    "                        X, y, left_idx, depth-1, alpha, beta, tree_depth+1)\n",
    "                    _, right_future = self.find_best_split_lookahead(\n",
    "                        X, y, right_idx, depth-1, alpha, beta, tree_depth+1)\n",
    "                    \n",
    "                    # Combine immediate and future (weighted average)\n",
    "                    future_mse = (len(left_idx) * left_future + len(right_idx) * right_future) / total_samples\n",
    "                    \n",
    "                    # Weight immediate vs future performance\n",
    "                    combined_score = 0.6 * immediate_mse + 0.4 * future_mse\n",
    "                else:\n",
    "                    combined_score = immediate_mse\n",
    "                \n",
    "                if combined_score < best_score:\n",
    "                    best_score = combined_score\n",
    "                    best_split = (feature, threshold, left_idx, right_idx)\n",
    "                \n",
    "                # Alpha-beta pruning\n",
    "                beta = min(beta, combined_score)\n",
    "                \n",
    "                # Optimistic bound check\n",
    "                left_bound = self.calculate_optimistic_bound(y, left_idx, depth-1)\n",
    "                right_bound = self.calculate_optimistic_bound(y, right_idx, depth-1)\n",
    "                optimistic_score = (len(left_idx) * left_bound + len(right_idx) * right_bound) / total_samples\n",
    "                \n",
    "                if optimistic_score >= beta or alpha >= beta:\n",
    "                    self.pruned_branches += 1\n",
    "                    break  # Prune remaining thresholds for this feature\n",
    "            \n",
    "            if alpha >= beta:\n",
    "                self.pruned_branches += 1\n",
    "                break  # Prune remaining features\n",
    "        \n",
    "        return best_split, best_score\n",
    "    \n",
    "    def fit(self, X, y, use_lookahead=True):\n",
    "        \"\"\"Fit the lookahead CART model.\"\"\"\n",
    "        # Start timing and memory tracking\n",
    "        tracemalloc.start()\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        self.evaluations = 0\n",
    "        self.pruned_branches = 0\n",
    "        \n",
    "        # Convert to numpy if pandas Series\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "        \n",
    "        # Build tree using breadth-first search\n",
    "        root = Node(depth=0, data_idx=np.arange(len(y)), y=y)\n",
    "        queue = Queue()\n",
    "        queue.put(root)\n",
    "        \n",
    "        while not queue.empty():\n",
    "            node = queue.get()\n",
    "            \n",
    "            if (node.depth >= self.max_depth or \n",
    "                len(node.data_idx) < 2 * self.min_leaf_samples):\n",
    "                continue\n",
    "            \n",
    "            # Find best split\n",
    "            if use_lookahead:\n",
    "                best_split, best_score = self.find_best_split_lookahead(\n",
    "                    X, y, node.data_idx, self.lookahead_depth, \n",
    "                    float('-inf'), float('inf'), node.depth)\n",
    "            else:\n",
    "                best_split, best_score = self.find_best_split_greedy(\n",
    "                    X, y, node.data_idx)\n",
    "            \n",
    "            if best_split is not None:\n",
    "                feature, threshold, left_idx, right_idx = best_split\n",
    "                \n",
    "                # Create child nodes\n",
    "                left_node = Node(node.depth + 1, left_idx, y, node, (feature, threshold, True))\n",
    "                right_node = Node(node.depth + 1, right_idx, y, node, (feature, threshold, False))\n",
    "                \n",
    "                node.left = left_node\n",
    "                node.right = right_node\n",
    "                \n",
    "                queue.put(left_node)\n",
    "                queue.put(right_node)\n",
    "        \n",
    "        self.tree = root\n",
    "        \n",
    "        # Record timing and memory\n",
    "        end_time = time.perf_counter()\n",
    "        current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        \n",
    "        self.fit_time = end_time - start_time\n",
    "        self.peak_memory = peak_memory\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the fitted tree.\"\"\"\n",
    "        def predict_row(row, node):\n",
    "            while node.left is not None and node.right is not None:\n",
    "                feature, threshold, _ = node.left.condition\n",
    "                if row[feature] <= threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            return node.pred\n",
    "        \n",
    "        if hasattr(X, 'iterrows'):\n",
    "            return np.array([predict_row(row, self.tree) for _, row in X.iterrows()])\n",
    "        else:\n",
    "            return np.array([predict_row(X.iloc[i], self.tree) for i in range(len(X))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47545636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xor_regression_data(n=1500, noise_level=0.2, random_state=123):\n",
    "    \"\"\"Create XOR regression dataset like the original.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    X = pd.DataFrame({\n",
    "        'X1': np.random.randint(0, 2, n),\n",
    "        'X2': np.random.randint(0, 2, n), \n",
    "        'X3': np.random.randn(n),\n",
    "        'X4': np.random.randn(n)\n",
    "    })\n",
    "    \n",
    "    # XOR relationship: X1 XOR X2 plus noise\n",
    "    y = ((X['X1'] ^ X['X2']) + noise_level * np.random.randn(n)).astype(float)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_complex_interaction_data(n=1000, random_state=42):\n",
    "    \"\"\"Create dataset with more complex interactions.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    X = pd.DataFrame({\n",
    "        'A': np.random.uniform(-2, 2, n),\n",
    "        'B': np.random.uniform(-2, 2, n),\n",
    "        'C': np.random.uniform(-2, 2, n),\n",
    "        'D': np.random.randn(n),\n",
    "        'E': np.random.randn(n)\n",
    "    })\n",
    "    \n",
    "    # Complex interaction: (A > 0 AND B > 0) creates different relationship with C\n",
    "    y = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if X.iloc[i]['A'] > 0 and X.iloc[i]['B'] > 0:\n",
    "            y[i] = 2.0 * X.iloc[i]['C'] + 0.1 * np.random.randn()\n",
    "        elif X.iloc[i]['A'] < 0 and X.iloc[i]['B'] < 0:\n",
    "            y[i] = -1.5 * X.iloc[i]['C'] + 0.1 * np.random.randn()\n",
    "        else:\n",
    "            y[i] = 0.5 * X.iloc[i]['C'] + 0.3 * np.random.randn()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4fe45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_memory(bytes_value):\n",
    "    \"\"\"Format memory in human readable form.\"\"\"\n",
    "    if bytes_value < 1024:\n",
    "        return f\"{bytes_value} B\"\n",
    "    elif bytes_value < 1024**2:\n",
    "        return f\"{bytes_value/1024:.1f} KB\"\n",
    "    else:\n",
    "        return f\"{bytes_value/(1024**2):.1f} MB\"\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format time in human readable form.\"\"\"\n",
    "    if seconds < 1e-3:\n",
    "        return f\"{seconds*1e6:.1f} μs\"\n",
    "    elif seconds < 1:\n",
    "        return f\"{seconds*1e3:.1f} ms\"\n",
    "    else:\n",
    "        return f\"{seconds:.3f} s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9855b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOOKAHEAD CART vs SKLEARN BASELINE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "XOR Regression\n",
      "--------------------------------------------------\n",
      "Train size: 1050, Test size: 450, Features: 4\n",
      "\n",
      "Sklearn DecisionTreeRegressor (Greedy Baseline):\n",
      "  MSE: 0.3157\n",
      "  Time: 2.1 ms\n",
      "\n",
      "Lookahead CART (depth=2):\n",
      "  MSE: 0.0397\n",
      "  Time: 376.8 ms\n",
      "  Memory: 139.4 KB\n",
      "  Evaluations: 312\n",
      "  Pruned branches: 208\n",
      "\n",
      "Comparison vs Sklearn:\n",
      "  MSE improvement: 0.2759\n",
      "  Time ratio: 178.90x\n",
      "  Pruning effectiveness: 40.0%\n",
      "  ✅ Lookahead provides significant improvement!\n",
      "\n",
      "Complex Interactions\n",
      "--------------------------------------------------\n",
      "Train size: 700, Test size: 300, Features: 5\n",
      "\n",
      "Sklearn DecisionTreeRegressor (Greedy Baseline):\n",
      "  MSE: 0.6408\n",
      "  Time: 1.9 ms\n",
      "\n",
      "Lookahead CART (depth=2):\n",
      "  MSE: 0.7151\n",
      "  Time: 2.283 s\n",
      "  Memory: 102.7 KB\n",
      "  Evaluations: 1772\n",
      "  Pruned branches: 1510\n",
      "\n",
      "Comparison vs Sklearn:\n",
      "  MSE improvement: -0.0742\n",
      "  Time ratio: 1223.84x\n",
      "  Pruning effectiveness: 46.0%\n",
      "  ⚠️ No significant improvement over sklearn\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average MSE improvement vs Sklearn: 0.1008\n",
      "Average time overhead: 701.37x\n",
      "Average pruning rate: 43.0%\n",
      "Total evaluations: 2084\n",
      "Total branches pruned: 1718\n",
      "\n",
      "Lookahead most effective on: XOR Regression\n",
      "  MSE improvement: 0.2759\n",
      "  Time overhead: 178.90x\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_comparison():\n",
    "    \"\"\"Compare lookahead CART vs sklearn DecisionTreeRegressor baseline.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"LOOKAHEAD CART vs SKLEARN BASELINE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    datasets = [\n",
    "        (\"XOR Regression\", lambda: create_xor_regression_data(1500, 0.2)),\n",
    "        (\"Complex Interactions\", lambda: create_complex_interaction_data(1000)),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for dataset_name, dataset_func in datasets:\n",
    "        print(f\"\\n{dataset_name}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = dataset_func()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}, Features: {X.shape[1]}\")\n",
    "        \n",
    "        # Sklearn DecisionTreeRegressor (Greedy Baseline)\n",
    "        print(\"\\nSklearn DecisionTreeRegressor (Greedy Baseline):\")\n",
    "        sklearn_tree = DecisionTreeRegressor(max_depth=3, min_samples_leaf=10, random_state=42)\n",
    "        start_time = time.perf_counter()\n",
    "        sklearn_tree.fit(X_train, y_train)\n",
    "        sklearn_time = time.perf_counter() - start_time\n",
    "        \n",
    "        y_pred_sklearn = sklearn_tree.predict(X_test)\n",
    "        mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "        \n",
    "        print(f\"  MSE: {mse_sklearn:.4f}\")\n",
    "        print(f\"  Time: {format_time(sklearn_time)}\")\n",
    "        \n",
    "        # Our Lookahead CART\n",
    "        print(\"\\nLookahead CART (depth=2):\")\n",
    "        lookahead_cart = LookaheadCART(max_depth=3, lookahead_depth=2, min_leaf_samples=10)\n",
    "        lookahead_cart.fit(X_train, y_train, use_lookahead=True)\n",
    "        \n",
    "        y_pred_lookahead = lookahead_cart.predict(X_test)\n",
    "        mse_lookahead = mean_squared_error(y_test, y_pred_lookahead)\n",
    "        \n",
    "        print(f\"  MSE: {mse_lookahead:.4f}\")\n",
    "        print(f\"  Time: {format_time(lookahead_cart.fit_time)}\")\n",
    "        print(f\"  Memory: {format_memory(lookahead_cart.peak_memory)}\")\n",
    "        print(f\"  Evaluations: {lookahead_cart.evaluations}\")\n",
    "        print(f\"  Pruned branches: {lookahead_cart.pruned_branches}\")\n",
    "        \n",
    "        # Comparison\n",
    "        mse_improvement = mse_sklearn - mse_lookahead\n",
    "        time_ratio = lookahead_cart.fit_time / sklearn_time if sklearn_time > 0 else 1.0\n",
    "        pruning_rate = lookahead_cart.pruned_branches / (lookahead_cart.evaluations + lookahead_cart.pruned_branches) if (lookahead_cart.evaluations + lookahead_cart.pruned_branches) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nComparison vs Sklearn:\")\n",
    "        print(f\"  MSE improvement: {mse_improvement:.4f}\")\n",
    "        print(f\"  Time ratio: {time_ratio:.2f}x\")\n",
    "        print(f\"  Pruning effectiveness: {pruning_rate:.1%}\")\n",
    "        \n",
    "        if mse_improvement > 0.01:\n",
    "            print(f\"  ✅ Lookahead provides significant improvement!\")\n",
    "        elif mse_improvement > 0.001:\n",
    "            print(f\"  ≈ Modest improvement\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ No significant improvement over sklearn\")\n",
    "        \n",
    "        results.append({\n",
    "            'dataset': dataset_name,\n",
    "            'mse_sklearn': mse_sklearn,\n",
    "            'mse_lookahead': mse_lookahead,\n",
    "            'improvement': mse_improvement,\n",
    "            'time_ratio': time_ratio,\n",
    "            'pruning_rate': pruning_rate,\n",
    "            'evaluations': lookahead_cart.evaluations,\n",
    "            'pruned_branches': lookahead_cart.pruned_branches\n",
    "        })\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    avg_improvement = np.mean([r['improvement'] for r in results])\n",
    "    avg_time_ratio = np.mean([r['time_ratio'] for r in results])\n",
    "    avg_pruning_rate = np.mean([r['pruning_rate'] for r in results])\n",
    "    total_evaluations = sum([r['evaluations'] for r in results])\n",
    "    total_pruned = sum([r['pruned_branches'] for r in results])\n",
    "    \n",
    "    print(f\"Average MSE improvement vs Sklearn: {avg_improvement:.4f}\")\n",
    "    print(f\"Average time overhead: {avg_time_ratio:.2f}x\")\n",
    "    print(f\"Average pruning rate: {avg_pruning_rate:.1%}\")\n",
    "    print(f\"Total evaluations: {total_evaluations}\")\n",
    "    print(f\"Total branches pruned: {total_pruned}\")\n",
    "    \n",
    "    # Best case analysis\n",
    "    best_dataset = max(results, key=lambda x: x['improvement'])\n",
    "    print(f\"\\nLookahead most effective on: {best_dataset['dataset']}\")\n",
    "    print(f\"  MSE improvement: {best_dataset['improvement']:.4f}\")\n",
    "    print(f\"  Time overhead: {best_dataset['time_ratio']:.2f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = comprehensive_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6e596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Data Science)",
   "language": "python",
   "name": "py311ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
